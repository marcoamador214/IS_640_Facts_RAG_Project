{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student RAG Project - Guided Implementation\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this project, you'll build a **RAG (Retrieval-Augmented Generation)** system that can answer questions about your documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "- ‚úÖ File I/O (reading documents)\n",
    "- ‚úÖ String manipulation (text chunking)\n",
    "- ‚úÖ Functions and parameters\n",
    "- ‚úÖ Lists and dictionaries\n",
    "- ‚úÖ Loops and conditionals\n",
    "- ‚úÖ Basic calculations and statistics\n",
    "\n",
    "### What's Provided for You:\n",
    "- ‚úÖ Embedding model (converts text to numbers)\n",
    "- ‚úÖ Vector database (stores and searches embeddings)\n",
    "- ‚úÖ LLM connection (generates answers)\n",
    "\n",
    "### Your Tasks:\n",
    "You'll complete **TODO sections** marked with `# TODO:` comments.\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking setup...\n",
      "‚úì chromadb is installed\n",
      "‚úì sentence_transformers is installed\n",
      "‚úì requests is installed\n",
      "\n",
      "‚úì All required packages are installed!\n",
      "You're ready to start!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pre-built helper module\n",
    "from rag_helpers import (\n",
    "    EmbeddingModel,\n",
    "    VectorDatabase,\n",
    "    LLM,\n",
    "    Timer,\n",
    "    print_separator,\n",
    "    print_search_results,\n",
    "    print_rag_answer,\n",
    "    check_setup\n",
    ")\n",
    "\n",
    "# Import standard Python libraries you'll use\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Check if everything is installed correctly\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Set up the basic settings for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Documents folder: ./my_docs\n",
      "  Chunk size: 500 characters\n",
      "  Overlap: 50 characters\n",
      "  Top-K results: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change this to point to YOUR documents folder\n",
    "DOCS_FOLDER = \"./my_docs\"\n",
    "\n",
    "# Chunking settings (you can experiment with these!)\n",
    "CHUNK_SIZE = 500      # How many characters per chunk\n",
    "OVERLAP = 50          # How many characters overlap between chunks\n",
    "\n",
    "# How many results to retrieve for each query\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Documents folder: {DOCS_FOLDER}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"  Overlap: {OVERLAP} characters\")\n",
    "print(f\"  Top-K results: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #1: Document Loading\n",
    "\n",
    "**Your Task:** Write a function to load all text files from a folder.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through all `.txt` files in the folder\n",
    "2. Read each file's content\n",
    "3. Store the content and filename in a dictionary\n",
    "4. Return a list of these dictionaries\n",
    "\n",
    "**Python concepts:** File I/O, loops, dictionaries, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 8 documents\n",
      "\n",
      "First document: 1_Dunning-Kruger_Facts.txt\n",
      "Content preview: Title: The Dunning‚ÄìKruger Effect ‚Äî Why Incompetence Feels Like Confidence\n",
      "\n",
      "People often assume confidence means competence, but research shows otherwise. The Dunning‚ÄìKruger Effect describes how people...\n"
     ]
    }
   ],
   "source": [
    "def load_documents(folder_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load all text documents from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing .txt files\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'content': the text content of the file\n",
    "        - 'filename': the name of the file\n",
    "\n",
    "    Example:\n",
    "        [\n",
    "            {'content': 'This is doc 1...', 'filename': 'doc1.txt'},\n",
    "            {'content': 'This is doc 2...', 'filename': 'doc2.txt'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty list to store documents\n",
    "    # 2. Use Path(folder_path).glob(\"*.txt\") to find all .txt files\n",
    "    # 3. For each file:\n",
    "    #    - Open it with open(file_path, 'r', encoding='utf-8')\n",
    "    #    - Read the content with .read()\n",
    "    #    - Create a dictionary with 'content' and 'filename'\n",
    "    #    - Append to your list\n",
    "    # 4. Return the list\n",
    "\n",
    "    documents = []  # Start with empty list\n",
    "\n",
    "    # Your code here:\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Create a dictionary\n",
    "        our_dict = {\n",
    "            'content': content,\n",
    "            'filename': file_path.name\n",
    "        }\n",
    "\n",
    "        # Add to documents list\n",
    "        documents.append(our_dict)\n",
    "\n",
    "    print(f\"‚úì Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Test your function\n",
    "documents = load_documents(DOCS_FOLDER)\n",
    "\n",
    "# Display first document (if any were loaded)\n",
    "if documents:\n",
    "    print(f\"\\nFirst document: {documents[0]['filename']}\")\n",
    "    print(f\"Content preview: {documents[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents loaded! Check your folder path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #2: Text Chunking Function\n",
    "\n",
    "**Your Task:** Write a function to split long text into smaller chunks with overlap.\n",
    "\n",
    "**Why?** Long documents are too big for embeddings. We need to split them into smaller pieces.\n",
    "\n",
    "**What to do:**\n",
    "1. Start at the beginning of the text\n",
    "2. Take a chunk of `chunk_size` characters\n",
    "3. Move forward by `chunk_size - overlap` characters\n",
    "4. Repeat until you reach the end\n",
    "\n",
    "**Python concepts:** String slicing, loops, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text length: 800 characters\n",
      "Number of chunks: 10\n",
      "\n",
      "First chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n",
      "Second chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text to split\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: How many characters to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a long document...\"\n",
    "        chunks = chunk_text(text, chunk_size=100, overlap=20)\n",
    "        # Returns: ['This is a long...', 'long document...']\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty list to store chunks\n",
    "    # 2. Start with position = 0\n",
    "    # 3. While position < len(text):\n",
    "    #    - Extract chunk from position to position+chunk_size\n",
    "    #    - Add chunk to list (if not empty)\n",
    "    #    - Move position forward by (chunk_size - overlap)\n",
    "    # 4. Return the list of chunks\n",
    "\n",
    "    chunks = []  # Start with empty list\n",
    "    position = 0  # Start at beginning\n",
    "\n",
    "    # Your code here:\n",
    "    while position < len(text):\n",
    "        # Extract a chunk\n",
    "        chunk = text[position : position + chunk_size]\n",
    "\n",
    "        # Add to chunks list\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Move position forward\n",
    "        position += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test your chunking function\n",
    "test_text = \"This is a test. \" * 50  # Create a long test string\n",
    "test_chunks = chunk_text(test_text, chunk_size=100, overlap=20)\n",
    "\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "print(f\"Number of chunks: {len(test_chunks)}\")\n",
    "print(f\"\\nFirst chunk: {test_chunks[0]}\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"Second chunk: {test_chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #3: Process All Documents into Chunks\n",
    "\n",
    "**Your Task:** Use your chunking function to split ALL documents into chunks and create metadata.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through each document\n",
    "2. Chunk the document's content\n",
    "3. For each chunk, create metadata (which file it came from, which chunk number)\n",
    "4. Store everything in a list\n",
    "\n",
    "**Python concepts:** Nested loops, dictionaries, enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 24 chunks from 8 documents\n",
      "\n",
      "Example chunk:\n",
      "  Source: 1_Dunning-Kruger_Facts.txt\n",
      "  Chunk ID: 0\n",
      "  Text: Title: The Dunning‚ÄìKruger Effect ‚Äî Why Incompetence Feels Like Confidence\n",
      "\n",
      "People often assume confidence means competence, but research shows otherwise. The Dunning‚ÄìKruger Effect describes how people...\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def process_documents(documents: List[Dict[str, str]],\n",
    "                     chunk_size: int = 500,\n",
    "                     overlap: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Process all documents into chunks with metadata.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries\n",
    "        chunk_size: Size of each chunk\n",
    "        overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (chunk_texts, chunk_metadatas)\n",
    "        - chunk_texts: List of chunk strings\n",
    "        - chunk_metadatas: List of metadata dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create two empty lists: chunk_texts and chunk_metadatas\n",
    "    # 2. For each document:\n",
    "    #    - Get the document's content and filename\n",
    "    #    - Use your chunk_text() function to split it\n",
    "    #    - For each chunk (use enumerate to get index):\n",
    "    #      - Add chunk text to chunk_texts\n",
    "    #      - Create metadata dict with 'source' and 'chunk_id'\n",
    "    #      - Add metadata to chunk_metadatas\n",
    "    # 3. Return both lists as a tuple\n",
    "\n",
    "    chunk_texts = []\n",
    "    chunk_metadatas = []\n",
    "\n",
    "    # Your code here:\n",
    "    for doc in documents:\n",
    "        filename = doc[\"filename\"] # Get document content and filename\n",
    "        content = doc[\"content\"]\n",
    "        \n",
    "        chunks = chunk_text(content, chunk_size=chunk_size, overlap=overlap) # Chunk the document\n",
    "\n",
    "        # For each chunk:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_texts.append(chunk) #   - Add chunk text to chunk_texts\n",
    "\n",
    "        #   - Create metadata dictionary\n",
    "            metadata = {\n",
    "                    \"source\": filename,\n",
    "                    \"chunk_id\": i\n",
    "                }\n",
    "            \n",
    "        #   - Add metadata to chunk_metadatas\n",
    "            chunk_metadatas.append(metadata)\n",
    "\n",
    "    print(f\"‚úì Created {len(chunk_texts)} chunks from {len(documents)} documents\")\n",
    "    return chunk_texts, chunk_metadatas\n",
    "\n",
    "\n",
    "# Process all documents\n",
    "chunk_texts, chunk_metadatas = process_documents(documents, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "# Display example\n",
    "if chunk_texts:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Source: {chunk_metadatas[0]['source']}\")\n",
    "    print(f\"  Chunk ID: {chunk_metadatas[0]['chunk_id']}\")\n",
    "    print(f\"  Text: {chunk_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Built: Create Embeddings and Store in Database\n",
    "\n",
    "This part uses the pre-built helpers. Just run these cells - no coding needed! ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úì Model loaded!\n",
      "\n",
      "Creating embeddings...\n",
      "Embedding 24 texts...\n",
      "‚úì Complete!\n",
      "‚úì Created 24 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model (pre-built)\n",
    "print(\"Initializing embedding model...\")\n",
    "embedder = EmbeddingModel()\n",
    "\n",
    "# Create embeddings for all chunks (pre-built)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "embeddings = embedder.embed_multiple(chunk_texts)\n",
    "print(f\"‚úì Created {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector database...\n",
      "‚úì Vector database initialized\n",
      "  Collection: student_rag\n",
      "  Current documents: 24\n",
      "\n",
      "Adding chunks to database...\n",
      "‚úì Added 24 chunks to database\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database (pre-built)\n",
    "print(\"Initializing vector database...\")\n",
    "vector_db = VectorDatabase()\n",
    "\n",
    "# Add chunks to database (pre-built)\n",
    "print(\"\\nAdding chunks to database...\")\n",
    "vector_db.add_chunks(chunk_texts, embeddings, chunk_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama LLM...\n",
      "‚úì LLM initialized: gemma3:1b-it-qat at http://127.0.0.1:11434\n",
      "\n",
      "Testing LLM connection...\n",
      "‚úì LLM is working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM connection (pre-built)\n",
    "print(\"Connecting to Ollama LLM...\")\n",
    "llm = LLM(model=\"gemma3:1b-it-qat\")\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting LLM connection...\")\n",
    "if llm.test_connection():\n",
    "    print(\"‚úì LLM is working!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LLM connection failed! Make sure Docker container is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #4: RAG Query Function\n",
    "\n",
    "**Your Task:** Write the main RAG function that ties everything together!\n",
    "\n",
    "**What to do:**\n",
    "1. Embed the user's question\n",
    "2. Search the database for similar chunks\n",
    "3. Build a prompt with the retrieved context\n",
    "4. Ask the LLM to answer based on the context\n",
    "5. Return the answer and metadata\n",
    "\n",
    "**Python concepts:** Functions, string formatting, dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG query function defined!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def rag_query(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "    Args:\n",
    "        question: The user's question\n",
    "        top_k: How many chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'question': the original question\n",
    "        - 'answer': the LLM's answer\n",
    "        - 'sources': list of source filenames\n",
    "        - 'contexts': list of retrieved chunks\n",
    "        - 'time': how long it took\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timer\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    # TODO: Implement the RAG pipeline!\n",
    "    # HINTS:\n",
    "    # 1. Embed the question using: embedder.embed_text(question)\n",
    "    # 2. Search database using: vector_db.search(query_embedding, top_k)\n",
    "    # 3. Extract retrieved chunks and metadata from search results:\n",
    "    #    - retrieved_chunks = results['documents'][0]\n",
    "    #    - retrieved_metadata = results['metadatas'][0]\n",
    "    # 4. Build context by joining chunks with newlines\n",
    "    # 5. Create prompt (template below)\n",
    "    # 6. Generate answer using: llm.generate_answer(prompt)\n",
    "    # 7. Extract source filenames from metadata\n",
    "    # 8. Return everything in a dictionary\n",
    "\n",
    "    # Step 1: Embed question\n",
    "    query_embedding = embedder.embed_text(question)\n",
    "\n",
    "    # Step 2: Search database\n",
    "    results = vector_db.search(query_embedding, top_k=top_k)\n",
    "\n",
    "    # Step 3: Extract results\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    retrieved_metadata = results[\"metadatas\"][0]\n",
    "\n",
    "    # Step 4: Build context\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Step 5: Create prompt (use this template)\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 6: Generate answer\n",
    "    answer = llm.generate_answer(prompt)\n",
    "\n",
    "    # Step 7: Extract sources\n",
    "    # sources = [m.get(\"source\", \"unknown\") for m in retrieved_metadata]  # Your code here (get 'source' from each metadata dict)\n",
    "    # seen = set()\n",
    "    # sources = []\n",
    "    # for s in sources:\n",
    "    #     if s not in seen:\n",
    "    #         seen.add(s)\n",
    "    #         sources.append(s)\n",
    "    seen = set()\n",
    "    sources = []\n",
    "    for m in retrieved_metadata:\n",
    "        src = m.get(\"source\", \"unknown\")\n",
    "        if src not in seen:\n",
    "            seen.add(src)\n",
    "            sources.append(src)\n",
    "\n",
    "    # Stop timer\n",
    "    elapsed_time = timer.stop()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'contexts': retrieved_chunks,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì RAG query function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your RAG System!\n",
    "\n",
    "Let's try asking some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== RAG ANSWER ========================\n",
      "\n",
      "QUESTION: What are the attendance rules?\n",
      "\n",
      "ANSWER:\n",
      "The provided text does not discuss attendance rules. It focuses on the cognitive blind spot related to overconfidence and the impact of it on judgment.\n",
      "\n",
      "SOURCES: 6_Premortem_Technique_and_Plan_Confidence.txt, 1_Dunning-Kruger_Facts.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "TIME: 3.92 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test question 1\n",
    "result = rag_query(\"What are the attendance rules?\")\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== RAG ANSWER ========================\n",
      "\n",
      "QUESTION: What happens if you cheat?\n",
      "\n",
      "ANSWER:\n",
      "The context doesn‚Äôt address what happens if someone cheats. It focuses on the dangers of overconfidence and the different types of overconfidence.\n",
      "\n",
      "SOURCES: 5_Overconfidence_and_Leadership_Selection_Risks.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "TIME: 4.03 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Try your own question!\n",
    "my_question = \"What happens if you cheat?\"  # Change this!\n",
    "\n",
    "result = rag_query(my_question)\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #5: Create Test Dataset\n",
    "\n",
    "**Your Task:** Create a list of test questions to evaluate your RAG system.\n",
    "\n",
    "**What to do:**\n",
    "1. Think of 10 questions your documents can answer\n",
    "2. For each question, write the expected answer\n",
    "3. Store them in a structured format\n",
    "\n",
    "**Python concepts:** Lists, dictionaries, data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 16 test questions\n",
      "\n",
      "Example question:\n",
      "  Q: What is the Dunning‚ÄìKruger Effect?\n",
      "  Expected: People with low skill overestimate their ability because they lack insight into their own incompetence\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    {\n",
    "        'question': 'What is the Dunning‚ÄìKruger Effect?',\n",
    "        'expected_answer': 'People with low skill overestimate their ability because they lack insight into their own incompetence',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What are the three main types of overconfidence described by Moore and Healy?',\n",
    "        'expected_answer': 'Overestimation, overplacement, and overprecision',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How is overprecision defined in the readings?',\n",
    "        'expected_answer': 'Being too certain that your beliefs or estimates are correct, with intervals that are too narrow',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does overestimation differ from overplacement?',\n",
    "        'expected_answer': 'Overestimation is overrating your own performance in absolute terms, while overplacement is thinking you rank higher than others',\n",
    "        'category': 'conceptual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Why can overconfidence lead to bad decisions according to the texts?',\n",
    "        'expected_answer': 'It can cause people to take excessive risks, ignore feedback, or fail to prepare because they think they are already right',\n",
    "        'category': 'explanatory'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What role does feedback play in correcting overconfidence?',\n",
    "        'expected_answer': 'Accurate, timely feedback can help people recalibrate their beliefs and reduce overconfidence',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "    {\n",
    "        'question': 'According to the readings, in what kinds of tasks is overconfidence especially common?',\n",
    "        'expected_answer': 'Difficult or ambiguous tasks where people cannot easily see their own mistakes',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does experience or expertise affect overconfidence?',\n",
    "        'expected_answer': 'More experience can reduce some forms of overconfidence, but experts can still show overprecision',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is miscalibration in the context of confidence judgments?',\n",
    "        'expected_answer': 'A mismatch between stated confidence levels and actual accuracy, such as being 90% confident but only right 60% of the time',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What strategies do the readings suggest for reducing overconfidence?',\n",
    "        'expected_answer': 'Considering alternative explanations, seeking disconfirming evidence, or using more structured forecasting methods',\n",
    "        'category': 'application'\n",
    "    },\n",
    "    {\n",
    "        'question': 'True or false, does overprecision mean being too sure that your answer is correct?',\n",
    "        'expected_answer': 'True'\n",
    "    },\n",
    "    {\n",
    "        'question': 'True or false, does hearing the same statement many times make it seem true?',\n",
    "        'expected_answer': 'True'\n",
    "    },\n",
    "    {\n",
    "        'question': 'True or false, people judge ideas only by evidence, not by how confidently they‚Äôre presented?',\n",
    "        'expected_answer': 'False'\n",
    "    },\n",
    "    {\n",
    "        'question': 'True or false, high performers are usually the most confident because they fully recognize how much better they are than others?',\n",
    "        'expected_answer': 'False'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is the following statement true or false: Misjudging someone‚Äôs confidence as competence is rare in group settings because people usually focus on evidence over presentation style.',\n",
    "        'expected_answer': 'False'\n",
    "    },\n",
    "     {\n",
    "        'question': 'Select a text source at random and give me its title and a short summary.',\n",
    "        'expected_answer': 'The Dunning‚ÄìKruger Effect ‚Äî Why Incompetence Feels Like Confidence;'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úì Created {len(test_questions)} test questions\")\n",
    "print(f\"\\nExample question:\")\n",
    "print(f\"  Q: {test_questions[0]['question']}\")\n",
    "print(f\"  Expected: {test_questions[0]['expected_answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #6: Calculate Evaluation Metrics\n",
    "\n",
    "**Your Task:** Write functions to measure how well your RAG system performs.\n",
    "\n",
    "**Python concepts:** Functions, calculations, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def calculate_average_latency(results: List[Dict]) -> float:\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Extract all 'time' values from results\n",
    "    # 2. Sum them up\n",
    "    # 3. Divide by the number of results\n",
    "    # 4. Return the average\n",
    "\n",
    "    # Your code here:\n",
    "    \"\"\"\n",
    "    Calculate average response time.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries (each has 'time' field)\n",
    "\n",
    "    Returns:\n",
    "        Average time in seconds\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "\n",
    "    total_time = 0.0\n",
    "    for r in results:\n",
    "        total_time += r.get('time', 0.0)\n",
    "\n",
    "    avg_time = total_time / len(results)\n",
    "    return avg_time\n",
    "\n",
    "\n",
    "\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Start with count = 0\n",
    "    # 2. For each result:\n",
    "    #    - Check if 'contexts' is not empty\n",
    "    #    - If yes, increment count\n",
    "    # 3. Return count\n",
    "\n",
    "    # Your code here:\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for r in results:\n",
    "        contexts = r.get('contexts', [])\n",
    "        if contexts:  # non-empty list means we retrieved something\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def get_all_sources(results: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get unique list of all sources used.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of unique source filenames\n",
    "    \"\"\"\n",
    "    all_sources = set()\n",
    "\n",
    "    for r in results:\n",
    "        sources = r.get('sources', [])\n",
    "        for s in sources:\n",
    "            all_sources.add(s)\n",
    "\n",
    "    return list(all_sources)\n",
    "    # Collect all sources\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #7: Run Complete Evaluation\n",
    "\n",
    "**Your Task:** Test your RAG system with all test questions and calculate metrics.\n",
    "\n",
    "**Python concepts:** Loops, function calls, data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on all test questions...\n",
      "\n",
      "\n",
      "‚úì Completed 16 tests\n"
     ]
    }
   ],
   "source": [
    "def run_evaluation(test_questions: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run RAG system on all test questions.\n",
    "\n",
    "    Args:\n",
    "        test_questions: List of test question dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test in test_questions:\n",
    "        # Get the question text\n",
    "        question = test.get('question', '')\n",
    "\n",
    "        # Run RAG query\n",
    "        result = rag_query(question)\n",
    "\n",
    "        # Attach expected answer and category for reference\n",
    "        result['expected_answer'] = test.get('expected_answer', '')\n",
    "        result['category'] = test.get('category', '')\n",
    "\n",
    "        # Store the result\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation on all test questions\n",
    "print(\"Running evaluation on all test questions...\\n\")\n",
    "all_results = run_evaluation(test_questions)\n",
    "\n",
    "print(f\"\\n‚úì Completed {len(all_results)} tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Results\n",
    "\n",
    "Show the evaluation metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== EVALUATION RESULTS ====================\n",
      "\n",
      "Total Questions Tested: 16\n",
      "Successful Retrievals: 16\n",
      "Hit Rate: 100.00%\n",
      "Average Latency: 4.11 seconds\n",
      "\n",
      "Sources Used: 4_Dominance_Signals_and_Perceived_Competence.txt, 6_Premortem_Technique_and_Plan_Confidence.txt, 7_Calibration_Training_and_Forecast_Accuracy.txt, 5_Overconfidence_and_Leadership_Selection_Risks.txt, 1_Dunning-Kruger_Facts.txt, 3_Illusory_Truth_Effect_Evidence.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics using your functions\n",
    "avg_latency = calculate_average_latency(all_results)\n",
    "successful_retrievals = count_successful_retrievals(all_results)\n",
    "all_sources_used = get_all_sources(all_results)\n",
    "hit_rate = successful_retrievals / len(all_results) if all_results else 0\n",
    "\n",
    "# Display metrics\n",
    "print_separator(\"EVALUATION RESULTS\")\n",
    "print(f\"\\nTotal Questions Tested: {len(all_results)}\")\n",
    "print(f\"Successful Retrievals: {successful_retrievals}\")\n",
    "print(f\"Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"\\nSources Used: {', '.join(all_sources_used)}\")\n",
    "print_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual Test Results:\n",
      "\n",
      "[Test 1]\n",
      "Question: What is the Dunning‚ÄìKruger Effect?\n",
      "Answer: The Dunning‚ÄìKruger Effect describes how people with low ability in a skill tend to overestimate their knowledge or performance.\n",
      "Sources: 1_Dunning-Kruger_Facts.txt\n",
      "Time: 4.86s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 2]\n",
      "Question: What are the three main types of overconfidence described by Moore and Healy?\n",
      "Answer: Overestimation, overplacement, and overprecision.\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 2.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 3]\n",
      "Question: How is overprecision defined in the readings?\n",
      "Answer: Overprecision happens when people are too certain about their beliefs or predictions, leaving too little room for error.\n",
      "Sources: 7_Calibration_Training_and_Forecast_Accuracy.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 3.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 4]\n",
      "Question: How does overestimation differ from overplacement?\n",
      "Answer: Overestimation is when people believe their performance is higher than it really is, such as a student expecting an A when they earned a C. Overplacement is thinking we are better than others, like believing we drive better than the average person.\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 4.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 5]\n",
      "Question: Why can overconfidence lead to bad decisions according to the texts?\n",
      "Answer: According to the texts, overconfidence can lead to risky decisions because individuals who trust intuition over data make decisions despite being average in competence. This can affect entire systems.\n",
      "Sources: 5_Overconfidence_and_Leadership_Selection_Risks.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 4.10s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 6]\n",
      "Question: What role does feedback play in correcting overconfidence?\n",
      "Answer: Feedback plays a role in correcting overconfidence. The text highlights that awareness of overconfidence types can help teams plan more cautiously and value humility as a strength. It suggests that understanding these distinctions is crucial for recognizing and addressing overconfidence.\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 4.41s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 7]\n",
      "Question: According to the readings, in what kinds of tasks is overconfidence especially common?\n",
      "Answer: Overconfidence is especially common in tasks where people often feel like they‚Äôre ‚Äúfeeling right‚Äù because their brains reward decisiveness and certainty.\n",
      "\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 3.62s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 8]\n",
      "Question: How does experience or expertise affect overconfidence?\n",
      "Answer: The text suggests that experience or expertise helps explain why confident errors are so common. People rarely feel like they‚Äôre guessing; they often *feel right* because their brains reward decisiveness and certainty.\n",
      "Sources: 5_Overconfidence_and_Leadership_Selection_Risks.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 3.19s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 9]\n",
      "Question: What is miscalibration in the context of confidence judgments?\n",
      "Answer: Miscalibration refers to the process of aligning confidence levels with actual results, comparing predictions and feedback to actual results. It‚Äôs the difference between feeling confident and actually being correct.\n",
      "Sources: 4_Dominance_Signals_and_Perceived_Competence.txt, 7_Calibration_Training_and_Forecast_Accuracy.txt, 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 4.41s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 10]\n",
      "Question: What strategies do the readings suggest for reducing overconfidence?\n",
      "Answer: The readings suggest that teams should plan more cautiously and value humility as a strength.\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 3.27s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 11]\n",
      "Question: True or false, does overprecision mean being too sure that your answer is correct?\n",
      "Answer: True. The text explicitly states that overprecision means being too sure about your beliefs or predictions, leaving too little room for error.\n",
      "Sources: 2_Overconfidence_Types_Moore_Healy.txt\n",
      "Time: 4.07s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 12]\n",
      "Question: True or false, does hearing the same statement many times make it seem true?\n",
      "Answer: True.\n",
      "Sources: 3_Illusory_Truth_Effect_Evidence.txt\n",
      "Time: 3.36s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 13]\n",
      "Question: True or false, people judge ideas only by evidence, not by how confidently they‚Äôre presented?\n",
      "Answer: False.\n",
      "\n",
      "The context explicitly states that ‚ÄúEffective decision-making requires separating how something is said from what is actually true.‚Äù It also highlights that people may believe they can finish a project without support, underestimating challenges, and even give advice confidently about topics they barely know.\n",
      "Sources: 1_Dunning-Kruger_Facts.txt, 3_Illusory_Truth_Effect_Evidence.txt, 4_Dominance_Signals_and_Perceived_Competence.txt\n",
      "Time: 5.56s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 14]\n",
      "Question: True or false, high performers are usually the most confident because they fully recognize how much better they are than others?\n",
      "Answer: False.\n",
      "\n",
      "The text indicates that high performers often underestimate their competence because they assume others find tasks equally easy.\n",
      "Sources: 1_Dunning-Kruger_Facts.txt, 4_Dominance_Signals_and_Perceived_Competence.txt\n",
      "Time: 3.95s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 15]\n",
      "Question: Is the following statement true or false: Misjudging someone‚Äôs confidence as competence is rare in group settings because people usually focus on evidence over presentation style.\n",
      "Answer: True.\n",
      "\n",
      "The context explicitly states that confident people often gain influence not because they're right, but because they *seem* right. This highlights that people focus on outward presentation style (body language, tone of voice, decisiveness) over actual performance.\n",
      "Sources: 1_Dunning-Kruger_Facts.txt, 4_Dominance_Signals_and_Perceived_Competence.txt\n",
      "Time: 4.53s\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 16]\n",
      "Question: Select a text source at random and give me its title and a short summary.\n",
      "Answer: ‚ÄúPerforming a Project Premortem.‚Äù\n",
      "\n",
      "The text discusses the phenomenon of confirmation bias, where people tend to believe information repeated multiple times is more likely to be true. It also highlights how this bias contributes to the persistence of myths and misinformation online.\n",
      "Sources: 6_Premortem_Technique_and_Plan_Confidence.txt, 3_Illusory_Truth_Effect_Evidence.txt, 7_Calibration_Training_and_Forecast_Accuracy.txt\n",
      "Time: 4.98s\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display individual results\n",
    "print(\"\\nIndividual Test Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"[Test {i}]\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:500]}\")\n",
    "    print(f\"Sources: {', '.join(set(result['sources']))}\")\n",
    "    print(f\"Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Your Results\n",
    "\n",
    "Save your test results to a JSON file for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Results saved to 'evaluation_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON file\n",
    "results_summary = {\n",
    "    'metrics': {\n",
    "        'total_questions': len(all_results),\n",
    "        'successful_retrievals': successful_retrievals,\n",
    "        'hit_rate': hit_rate,\n",
    "        'average_latency': avg_latency\n",
    "    },\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"‚úì Results saved to 'evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've successfully built a RAG system!\n",
    "\n",
    "### What You Accomplished:\n",
    "‚úÖ Loaded documents from files  \n",
    "‚úÖ Chunked text with overlap  \n",
    "‚úÖ Created a RAG query pipeline  \n",
    "‚úÖ Built a test dataset  \n",
    "‚úÖ Calculated evaluation metrics  \n",
    "‚úÖ Generated a results report  \n",
    "\n",
    "### Next Steps:\n",
    "- Try different chunk sizes and overlaps\n",
    "- Add more test questions\n",
    "- Experiment with different values for `top_k`\n",
    "- Analyze which questions work best\n",
    "- Write up your findings in a report\n",
    "\n",
    "### For Your Report:\n",
    "1. Describe your document collection\n",
    "2. Explain your chunking strategy\n",
    "3. Present your evaluation metrics\n",
    "4. Show examples of good and bad answers\n",
    "5. Discuss what you learned\n",
    "\n",
    "Great job! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
